# 2c
I found k=7 to be quite nice after some trial and error. This was found by simply running the script multiple times and "eyeballing it", but I also thought of this heuristic motivation. A polynomial of degree n will be able to change direction n-1 times and since [0,4pi] will cover two periods we need the polynomial to change direction 4 times => by this heuristic, it is reasonable to assume that k is at least 5 and for k<5 the model might underfit. However, trying this we see that the model struggles with the boundary conditions. Thus, by adding two more degrees of freedom (k=7) we create a model which can satisfy the two boundary conditions and also "turn" the correct amount of times. By increasing k sometimes the model does very well on training data because it can then be more expressive, but it generally seems like the MSE increases and this is presumably due to overfitting. The model does very well on the training data but doesn't at all generalize.

Note that this is definitely not exact, and sometimes k=7 behaves terribly and overfits, but from my limited testing it seems to work reasonably

# 2d 
After running the hyperparameter tuning I see that the plot definitely varies, but there are some consistent trends. Specifically, for low k values (1-4), the MSE seems to be quite high, and this is expected since in this area the model is presumably underfitting. It simply doesn't have enough expressivity to adjust well. However, for very high values of k (around 12-15) we also see that the MSE is high. This is presumably due to the model overfitting and thus it fits well on the training data, but doesn't generalize well. We know that for the k=15 model, it is possible to fit perfectly to 15 data points, but this is usually not a very good true approximation. Regarding the lambda parameter, it seems like for lower values of k, it doesn't matter a lot. This is expected since then the polynomial won't be overfitting anyway, but for higher k we see that the ridge regressions with higher regularisation generally overfit less so the MSE becomes lower. It is fairly noisy, though, but one can at least clearly see that for higher k, the lambda has a bigger impact than on low k.

When the test data is increased (15 train, 1000 test), the heatmap looks smoother. It also seems like the higher k polynomials are worse. This is presumably since overfitting now is more evident since there is more test data. For this split, there was also a big variance between runs: also not very surprising. When instead the train data was increased (1500 train, 10 test) it was way more consistent and it seems like for k in [7,11] all fit very well and is less sensitive to lambda. This makes sense since for this much training data, it is way harder to overfit. We also note that the MSE for the high k polynomials appears lower than for the previous version with way fewer training data points which is also a consequence of the fact that it's harder to overfit on a lot of data.

# 2e
It appears that for a low number of folds, there is quite a high mean MSE. This is reasonable since the polynomial only trains on a smaller fraction of the data and thus there might be values in the validation set not observed in the training. For higher number of folds, though, we see that this MSE generally decreases and it appears to converge to some value around 0.013. This is reasonable since training on 118 or 119 samples should be basically as informative so the mean MSE should be basically the same. The standard deviation also seems to stabilise, but looks like it might increase slightly. This is also reasonable since when there are more folds, there is less data to evaluate on, and thus there might be a higher variance in the evaluation metrics.